# -*- coding: utf-8 -*-
"""SBI_model1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ru_VyLs4QF8GC8znlW1q1C-UocwXOFOo
"""

BASE_FOLDER = '/content/drive/My Drive/sbi_project/'
from google.colab import drive
drive.mount('/content/drive/', force_remount = True)

!pip install torch_geometric
!pip install biopython
!pip install e3nn
!pip install Bio
!pip install transformers
!pip install fair-esm
!pip install e3nn --upgrade
!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.0.1+cu118.html # torch-geometric updated for torch 2.0.1

import os
import torch_geometric
from torch_geometric.data import Data
from Bio import PDB
import torch
import numpy as np
from scipy.spatial import distance_matrix, KDTree
from Bio.PDB import PDBParser, PPBuilder, SASA, Polypeptide, ShrakeRupley, is_aa
import numpy as np
import math
from Bio.PDB import PICIO, PDBIO
import esm
from typing import TypedDict, Dict, Tuple
import time
import re
from torch_geometric.loader import DataLoader
from sklearn.model_selection import train_test_split
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import TransformerConv, radius_graph
from e3nn import o3
from e3nn.nn import FullyConnectedNet
import pymol
from e3nn.nn.models.gate_points_2101 import Network as GatePointsNetwork
from joblib import Parallel, delayed
from torch_geometric.nn import radius
from multiprocessing import Pool
import torch_geometric.nn as pyg_nn

def compute_phi_psi_from_pdb(pdb_file: str):
    """Computes phi (ϕ) and psi (ψ) dihedral angles from a given PDB file."""

    # Parse the PDB file
    parser = PDB.PDBParser(QUIET=1)
    structure = parser.get_structure("protein", pdb_file)

    # Convert to internal coordinates
    structure.atom_to_internal_coordinates()

    # Extract first chain
    chain = list(structure.get_chains())[0]
    ic_chain = chain.internal_coord

    # Get dihedral angles
    d: Dict[
        Tuple[PDB.internal_coords.AtomKey,
              PDB.internal_coords.AtomKey,
              PDB.internal_coords.AtomKey,
              PDB.internal_coords.AtomKey],
        PDB.internal_coords.Dihedron
    ] = ic_chain.dihedra

    # Extract phi and psi angles
    phi_angles = {}
    psi_angles = {}

    for key, dihedral in d.items():
        atom1, atom2, atom3, atom4 = key  # Each is an AtomKey object

        # Extract residue numbers and atom names
        # Added error handling for cases with more than 2 underscores
        try:
            res1, aa1, atom_name1 = str(atom1).split("_")
            res2, aa2, atom_name2 = str(atom2).split("_")
            res3, aa3, atom_name3 = str(atom3).split("_")
            res4, aa4, atom_name4 = str(atom4).split("_")

            # Extract numerical part of residue ID using regex

            res1 = int(re.findall(r'\d+', res1)[0])
            res2 = int(re.findall(r'\d+', res2)[0])
            res3 = int(re.findall(r'\d+', res3)[0])
            res4 = int(re.findall(r'\d+', res4)[0])

        except (ValueError, IndexError):  # Handle both ValueError and potential IndexError
            # Skip this dihedral if it has an unexpected format
            continue

        # Identify PHI (ϕ): (C(i-1), N(i), CA(i), C(i))
        if atom_name1 == "C" and atom_name2 == "N" and atom_name3 == "CA" and atom_name4 == "C":
            phi_angles[res2] = dihedral.angle  # Phi belongs to residue `i` (res2)

        # Identify PSI (ψ): (N(i), CA(i), C(i), N(i+1))
        if atom_name1 == "N" and atom_name2 == "CA" and atom_name3 == "C" and atom_name4 == "N":
            psi_angles[res1] = dihedral.angle  # Psi belongs to residue `i` (res1)

    return phi_angles, psi_angles


def compute_sasa(pdb_file):
    """Compute Solvent Accessible Surface Area (SASA) for the protein."""

    # Parse the PDB file
    parser = PDBParser(QUIET=True)
    structure = parser.get_structure("protein", pdb_file)

    # Use Shrake-Rupley method to calculate SASA
    sr = ShrakeRupley()
    sr.compute(structure)  # This will compute the SASA values for the structure

    sasa = {}
    # Loop through all models, chains, and residues
    for model in structure:
        for chain in model:
            for residue in chain:
                if PDB.is_aa(residue):  # Only amino acids
                    # Access SASA through the residue's attribute `sasa`
                    if hasattr(residue, 'sasa') and residue.sasa is not None:  # Check if SASA is computed for this residue
                        residue_id = f"{chain.id}_{residue.get_resname()}_{residue.get_id()[1]}"
                        sasa_value = residue.sasa  # The SASA value computed by Shrake-Rupley
                        sasa[residue_id] = sasa_value  # Store SASA per residue

    return sasa

class GlobalAttentionLayer(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(GlobalAttentionLayer, self).__init__()
        self.attention = nn.Linear(in_channels, 1) # Learnable attention weights

    def forward(self, x, batch):
        # x: Node features, shape [num_nodes, in_channels]
        # batch: Batch vector, shape [num_nodes]

        # Compute attention weights for each node
        attn_weights = torch.sigmoid(self.attention(x))  # Sigmoid for 0-1 range
        # attn_weights = F.softmax(self.attention(x), dim=0) # Or softmax

        # Aggregate node features to form a global context vector
        global_context = pyg_nn.global_mean_pool(x * attn_weights, batch)  # or max_pool, add_pool

        # Apply attention, e.g., by concatenating with original node features
        x_attended = torch.cat([x, global_context[batch]], dim=-1)

        return x_attended

class MultiScaleEquivariantResidualNet(nn.Module):
    def __init__(self, node_dim=23, edge_dim=19, hidden_dim=64, num_layers=4):
        super().__init__()
        self.node_projection = nn.Linear(node_dim, hidden_dim)
        self.edge_projection = nn.Linear(edge_dim, hidden_dim)

        self.equiv_transformer = GatePointsNetwork(
            irreps_in=o3.Irreps(f"{node_dim}x0e") if node_dim else None,
            irreps_hidden=o3.Irreps([(hidden_dim, (0, 1))]),
            irreps_out=o3.Irreps(f"{hidden_dim}x0e"),
            irreps_node_attr=o3.Irreps("0e"),
            irreps_edge_attr=o3.Irreps(f"{hidden_dim}x0e"),
            layers=num_layers,
            max_radius=5.0,
            number_of_basis=16,
            radial_layers=2,
            radial_neurons=hidden_dim,
            num_neighbors=12.0,
            num_nodes=100.0,
            reduce_output=False
        )

        self.graph_transformer_local = TransformerConv(hidden_dim, hidden_dim, heads=8, concat=False, dropout=0.1)
        self.norm_local = nn.LayerNorm(hidden_dim)

        # **Global Attention Layer for Global Branch (GLOBAL)**
        self.global_attention = GlobalAttentionLayer(hidden_dim, hidden_dim * 2)
        self.norm_global = nn.LayerNorm(hidden_dim * 2) # Changed due to concatenation in GlobalAttentionLayer

        self.projection_head = FullyConnectedNet([hidden_dim * 2 + hidden_dim, hidden_dim, 128], act=torch.nn.ReLU())  # Adjusted input size
        self.classifier = nn.Linear(128, 1)

    def forward(self, data):
        edge_attr_proj = self.edge_projection(data.edge_attr)
        # Equivariant transformation to integrate structural features
        x = self.equiv_transformer({
            'pos': data.pos,
            'x': data.x,
            'edge_attr': edge_attr_proj,
            'batch': data.batch if hasattr(data, 'batch') else None
        })

        x_local = self.graph_transformer_local(x, data.edge_index)
        x_local = self.norm_local(x_local + x)

        # Global branch with global attention
        x_global = self.global_attention(x, data.batch)
        x_global = self.norm_global(x_global)

        # Concatenate both scales and pass through projection head
        x_concat = torch.cat([x_local, x_global], dim=-1)
        proj = self.projection_head(x_concat)
        out = self.classifier(proj)
        return out, proj

# Define root directory where all protein-ligand subfolders are stored
# root_dir = "demo"
root_dir = "refined-set"

# Function to extract binding residues from pocket PDB
def extract_binding_residues(pocket_pdb_file):
    parser = PDB.PDBParser(QUIET=True)
    structure = parser.get_structure("pocket", pocket_pdb_file)

    binding_residues = set()
    for chain in structure.get_chains():
        for residue in chain.get_residues():
            if PDB.is_aa(residue):  # Ensure it's an amino acid
                res_id = f"{chain.id}_{residue.get_resname()}_{residue.get_id()[1]}"
                binding_residues.add(res_id)

    return binding_residues


def load_pdb_as_graph(pdb_file, distance_threshold=8.0):
    if not os.path.exists(pdb_file):
        raise FileNotFoundError(f"PDB file {pdb_file} not found!")

    parser = PDBParser(QUIET=True)
    structure = parser.get_structure("protein", pdb_file)

    # Precompute SASA and dihedrals to avoid recomputing them for each residue
    sasa_values = compute_sasa(pdb_file)
    phi_angles, psi_angles = compute_phi_psi_from_pdb(pdb_file)

    nodes = []  # List of residue features
    positions = []  # C-alpha positions for graph edges
    residue_ids = [] # residue ids
    residue_indices = {}  # Map residue to index

    idx = 0
    for chain in structure.get_chains():
        for residue in chain.get_residues():
            if is_aa(residue):
                try:
                    ca = residue["CA"].get_coord()  # Extract C-alpha coordinates
                    positions.append(ca)
                    residue_indices[residue] = idx
                    nodes.append(extract_residue_features(residue, sasa_values, phi_angles, psi_angles))
                    idx += 1
                    residue_id = f"{chain.id}_{residue.get_resname()}_{residue.get_id()[1]}"
                    residue_ids.append(residue_id)
                except KeyError:
                    # Skip residues without C-alpha (e.g., missing density)
                    continue

    # Convert positions to NumPy array for faster processing
    positions = np.array(positions)

    # Use KDTree to find edges within the distance threshold
    tree = KDTree(positions)
    pairs = tree.query_pairs(distance_threshold)

    # Convert pairs to edge list
    # Convert to zero-based indices with validation
    edge_index = np.array(list(pairs)).T
    edge_index = edge_index.astype(np.int64)  # Ensure correct dtype

    # Validate indices before tensor conversion
    if np.any(edge_index >= len(positions)):
        invalid = edge_index[edge_index >= len(positions)]
        print(f"Invalid indices: {invalid}")
        edge_index = edge_index[edge_index < len(positions)]  # Filter invalid
    distances = np.linalg.norm(positions[edge_index[0]] - positions[edge_index[1]], axis=1)
    edge_attr = distances.reshape(-1, 1)

    # Generate radial basis features (expands 1D distance → 16D)
    edge_src, edge_dst = edge_index
    edge_vec = torch.tensor(positions)[edge_dst] - torch.tensor(positions)[edge_src]
    distances = torch.norm(edge_vec, dim=1)
    basis = radial_basis(distances, num_radial=16, max_radius=5.0)  # → [2783,16]

    # Add spherical harmonics for direction (degree=1 → 3D)
    edge_attr_sh = o3.spherical_harmonics([1], edge_vec, normalize=True)  # → [2783,3]

    edge_attr = torch.cat([basis, edge_attr_sh], dim=1)  # → [2783,16+3=19]

    x = torch.tensor(np.array(nodes), dtype=torch.float)
    edge_index = torch.tensor(edge_index, dtype=torch.long)
    edge_attr = torch.tensor(edge_attr, dtype=torch.float)

    # Add validation check
    max_node_idx = positions.shape[0] - 1  # Since indices start at 0
    if edge_index.max() > max_node_idx:
        raise ValueError(f"Invalid edge index {edge_index.max()} for {max_node_idx+1} nodes")

    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, pos = torch.tensor(positions)), residue_ids


def radial_basis(distances, num_radial, max_radius):
    """
    Encode distances using radial basis functions.

    Args:
    distances (torch.Tensor): Edge distances of shape [num_edges]
    num_radial (int): Number of radial basis functions
    max_radius (float): Maximum radius for normalization

    Returns:
    torch.Tensor: Encoded distances of shape [num_edges, num_radial]
    """
    # Create centers for radial basis functions
    centers = torch.linspace(0, max_radius, num_radial)

    # Compute width of Gaussian functions
    width = (max_radius / (num_radial - 1)) * 2

    # Expand dimensions for broadcasting
    distances = distances.unsqueeze(-1)
    centers = centers.unsqueeze(0)

    # Compute Gaussian radial basis functions
    rbf = torch.exp(-(distances - centers)**2 / width**2)

    return rbf


def extract_residue_features(residue, sasa_values, phi, psi):
    """Extracts structural features from a residue"""

    aa_types = ['ALA', 'ARG', 'ASN', 'ASP', 'CYS', 'GLN', 'GLU', 'GLY', 'HIS',
                'ILE', 'LEU', 'LYS', 'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRP', 'TYR', 'VAL']

    aa_one_hot = np.zeros(len(aa_types))
    if residue.get_resname() in aa_types:
        aa_one_hot[aa_types.index(residue.get_resname())] = 1  # One-hot encoding

    # Get residue ID for lookup in dictionaries
    residue_id = f"{residue.get_parent().id}_{residue.get_resname()}_{residue.get_id()[1]}"

    # Access SASA, phi, and psi using the residue ID
    sasa = sasa_values.get(residue_id, 0)  # Use 0 as default if not found
    phi_angle = phi.get(residue.get_id()[1], 0)  # Use 0 as default if not found
    psi_angle = psi.get(residue.get_id()[1], 0)  # Use 0 as default if not found

    # Concatenate all features into a single vector
    features = np.concatenate([aa_one_hot, [sasa, phi_angle, psi_angle]])

    return features

def process_complex(complex_dir):
    # Existing file identification code remains the same
    pdb_protein_file = None
    pdb_pocket_file = None
    mol2_file = None
    sdf_file = None

    for file in os.listdir(complex_dir):
        if file.endswith(".pdb"):
            if "pocket" in file.lower():
                pdb_pocket_file = os.path.join(complex_dir, file)
            else:
                pdb_protein_file = os.path.join(complex_dir, file)
        elif file.endswith(".mol2"):
            mol2_file = os.path.join(complex_dir, file)
        elif file.endswith(".sdf"):
            sdf_file = os.path.join(complex_dir, file)

    if not pdb_protein_file or not pdb_pocket_file:
        print(f"Skipping {complex_dir}: Missing required PDB files.")
        return None

    # Modified load_pdb_as_graph returns (graph, residue_ids)
    protein_graph, residue_ids = load_pdb_as_graph(pdb_protein_file)
    binding_residues = extract_binding_residues(pdb_pocket_file)

    # Add this debugging code
    num_nodes = protein_graph.x.size(0)
    max_index = protein_graph.edge_index.max().item()
    if max_index >= num_nodes:
        print(f"Warning: Graph from {complex_dir} has invalid edge indices.")
        print(f"Number of nodes: {num_nodes}, Max edge index: {max_index}")

    # Create labels using residue_ids comparison
    y = torch.tensor(
        [1 if rid in binding_residues else 0 for rid in residue_ids],
        dtype=torch.float
    ).view(-1, 1)

    # Add labels to graph while preserving original data
    protein_graph.y = y

    # Return all original values PLUS the labeled graph
    return protein_graph, binding_residues, mol2_file, sdf_file

def process_wrapper(complex_dir):
    return process_complex(complex_dir)

with Pool() as pool:
    complex_dirs = [os.path.join(BASE_FOLDER, root_dir, d) for d in os.listdir(BASE_FOLDER + root_dir) if os.path.isdir(os.path.join(BASE_FOLDER, root_dir, d))]
    all_complexes = pool.map(process_wrapper, complex_dirs)
    all_complexes = [result for result in all_complexes if result]

# Unpack results for different uses
dataset = [complex[0] for complex in all_complexes]  # Labeled graphs
binding_info = [complex[1] for complex in all_complexes]  # Original binding residues
ligand_files = [(complex[2], complex[3]) for complex in all_complexes]  # Mol2/SDF paths

# 3. Create dataset (using your existing loop)
# dataset = [process_complex(os.path.join(BASE_FOLDER, root_dir, name))
#           for name in os.listdir(BASE_FOLDER + root_dir) if os.path.isdir(os.path.join(BASE_FOLDER, root_dir, name))]
dataset = [g for g in dataset if g is not None]  # Remove None entries

# 4. Split dataset
train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)

# 5. Create data loaders
train_loader = DataLoader(train_data, batch_size=4, shuffle=True, follow_batch=['x'])
test_loader = DataLoader(test_data, batch_size=4, follow_batch=['x'])

model = MultiScaleEquivariantResidualNet(
    node_dim = 23,  # MUST match your feature size (20 AA + 3 features)
    edge_dim = 19,   # Distance-based edge attributes
    hidden_dim = 64,# Reduced for minimal example
    num_layers = 2
)

from sklearn.metrics import f1_score, roc_auc_score

# 7. Training setup
criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(5))  # Binary classification, is a given residue part of a binding site or not?
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

train_losses, val_accuracies = [], []

# 8. Training loop
for epoch in range(100):
    model.train()
    total_loss = 0
    for batch in train_loader:

        optimizer.zero_grad()
        # print(f"Batch shapes: x={batch.x.shape}, edge_index={batch.edge_index.shape}, edge_attr={batch.edge_attr.shape}")
        # print(f"Max node index: {batch.x.size(0)-1}, Max edge index: {batch.edge_index.max().item()}")
        assert batch.edge_index.max().item() < batch.x.size(0), "Invalid edge index!"
        try:
            out, _ = model(batch)
            loss = criterion(out, batch.y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        except RuntimeError as e:
            print(f"Error occurred: {e}")
            print(f"Batch data: {batch}")
            raise e
        # out, _ = model(batch)  # We only need the classification output
        # loss = criterion(out, batch.y)
        # loss.backward()
        # optimizer.step()
        # total_loss += loss.item()

    # Validation
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for batch in test_loader:
            out, _ = model(batch)
            preds = torch.sigmoid(out) > 0.5
            correct += (preds == batch.y).sum().item()
            total += batch.y.size(0)

    train_losses.append(total_loss/len(train_loader))
    val_accuracies.append(correct/total)

    print(f"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}, "
          f"Val Acc: {correct/total:.4f}")

# Assuming you've stored losses and accuracies during training
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(train_losses)
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.subplot(1, 2, 2)
plt.plot(val_accuracies)
plt.title('Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')

plt.tight_layout()
plt.show()

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_loader:
        out, _ = model(batch)
        preds = torch.sigmoid(out) > 0.5
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(batch.y.cpu().numpy())

all_preds = np.array(all_preds)
all_labels = np.array(all_labels)

from sklearn.metrics import confusion_matrix, classification_report

print(classification_report(all_labels, all_preds))
print(confusion_matrix(all_labels, all_preds))

from sklearn.metrics import roc_curve, auc

fpr, tpr, _ = roc_curve(all_labels, all_preds)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""False Negatives Are Costly

If the model fails to identify a true binding site (false negative), this could lead to missed drug targets or incorrect functional annotations.
In practical applications, missing a real binding site is usually worse than incorrectly predicting a non-binding site as a binding site (false positive).

This model is better than the other, because prioritizes minority class higher recall.
"""